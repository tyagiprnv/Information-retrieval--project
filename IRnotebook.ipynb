{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fasttext import FastVector\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os, glob, re, sys, random, unicodedata, collections\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.en.vec\n"
     ]
    }
   ],
   "source": [
    "eng_dictionary = FastVector(vector_file='wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.de.vec\n"
     ]
    }
   ],
   "source": [
    "ger_dictionary = FastVector(vector_file='wiki.de.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.fr.vec\n"
     ]
    }
   ],
   "source": [
    "fre_dictionary = FastVector(vector_file='wiki.fr.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_words = set(eng_dictionary.word2id.keys())\n",
    "ger_words = set(ger_dictionary.word2id.keys())\n",
    "fre_words = set(fre_dictionary.word2id.keys())\n",
    "overlap = list(eng_words & ger_words)\n",
    "overlap_fr_en = list(eng_words & fre_words)\n",
    "bilingual_dictionary = [(entry, entry) for entry in overlap]\n",
    "bilingual_dictionary_fr_en = [(entry, entry) for entry in overlap_fr_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.021914206256188045\n",
      "0.04893054014859706\n",
      "-0.019676202184651607\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"cow\"]\n",
    "fre_vector = fre_dictionary[\"vache\"]\n",
    "ger_vector = ger_dictionary[\"kuh\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_matrix, target_matrix = make_training_matrices(ger_dictionary, eng_dictionary, bilingual_dictionary)\n",
    "transform = learn_transformation(source_matrix, target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5374997100024711\n",
      "0.04893054014859706\n",
      "0.01941941296655649\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"cow\"]\n",
    "fre_vector = fre_dictionary[\"vache\"]\n",
    "ger_vector = ger_dictionary[\"kuh\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_matrix_fr, target_matrix_fr = make_training_matrices(fre_dictionary, eng_dictionary, bilingual_dictionary_fr_en)\n",
    "transform = learn_transformation(source_matrix_fr, target_matrix_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5374997100024711\n",
      "0.5894027260080459\n",
      "0.509233249968698\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"cow\"]\n",
    "fre_vector = fre_dictionary[\"vache\"]\n",
    "ger_vector = ger_dictionary[\"kuh\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7544904458022383\n",
      "0.6913910806895412\n",
      "0.7115484600503237\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"epidemiology\"]\n",
    "fre_vector = fre_dictionary[\"epidemiologie\"]\n",
    "ger_vector = ger_dictionary[\"epidemiologie\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgg = pd.DataFrame()\n",
    "with pd.read_json('livivo_medline_00.jsonl', lines=True, chunksize=10,nrows = 20) as readerq:\n",
    "    for chunkq in readerq:\n",
    "        dgg = dgg.append(chunkq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame()\n",
    "with pd.read_json('livivo_medline_00.jsonl', lines=True, chunksize=10000,nrows = 2000000) as reader:\n",
    "    for chunk in reader:\n",
    "        dff = dff.append(chunk[['DBRECORDID','TITLE','ABSTRACT','LANGUAGE']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dff.columns:\n",
    "    dff[i]=dff[i].apply(lambda x: x[0] if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=dff[dff['TITLE'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ger = dff.query('LANGUAGE == \"ger\"')\n",
    "df_eng = dff.query('LANGUAGE == \"eng\"')\n",
    "df_fre = dff.query('LANGUAGE == \"fre\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vector_t = eng_dictionary[\"medication\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_vector_t = fre_dictionary[\"médicament\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_vector1 = ger_dictionary[\"sind\"]\n",
    "ger_vector2 = ger_dictionary[\"neue\"]\n",
    "ger_vector3 = ger_dictionary[\"medikamente\"]\n",
    "ger_vector4 = ger_dictionary[\"zu\"]\n",
    "ger_vector5 = ger_dictionary[\"teuer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_vector_t = ger_vector1+ger_vector2+ger_vector3+ger_vector4+ger_vector5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47819758667529644\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(eng_vector_t, ger_vector_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4231839768240091\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(fre_vector_t, ger_vector_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text,lang):\n",
    "    \"\"\"Make all necessary preprocessing of text: strip accents and punctuation, remove the words only contains digit\n",
    "    remove \\n, tokenize our text, convert to lower case, remove stop words and \n",
    "    words with less than 2 chars.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "    str: cleaned tokenized text\n",
    "\n",
    "   \"\"\"    \n",
    "    WORD_MIN_LENGTH = 2\n",
    "    STOP_WORDS = nltk.corpus.stopwords.words(lang)\n",
    "    text = re.sub(re.compile('\\n'),' ',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in STOP_WORDS and len(word) >= WORD_MIN_LENGTH]\n",
    "    words = [word for word in words if word.isdigit()==False]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ger=(df_ger[\"TITLE\"].apply(lambda x: tokenize_text(x,'german')))\n",
    "tokenized_eng=(df_eng[\"TITLE\"].apply(lambda x: tokenize_text(x,'english')))\n",
    "tokenized_fre=(df_fre[\"TITLE\"].apply(lambda x: tokenize_text(x,'french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {\"de\": ger_dictionary,\n",
    "            \"en\": eng_dictionary,\n",
    "            \"fr\": fre_dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"herzkatheter\",\"echokardiographie\",\"krankenhausplanung\",\"ambient AND assisted AND living AND nursing\",\"low AND carb\",\"épidémiologie\",\"alzheimer AND demenz\",\"fatigue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(words):\n",
    "    \"\"\"Create a inverted index of words (tokens or terms) from a list of terms\n",
    "\n",
    "    Parameters:\n",
    "    words (list of str): tokenized document text\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"\n",
    "    inverted = {}\n",
    "    for index, word in enumerate(words):\n",
    "        locations = inverted.setdefault(word, [])\n",
    "        locations.append(index)\n",
    "    return inverted\n",
    "\n",
    "\n",
    "def inverted_index_add(inverted, doc_id, doc_index):\n",
    "    \"\"\"Insert document id into Inverted Index\n",
    "\n",
    "    Parameters:\n",
    "    inverted (dict): Inverted Index\n",
    "    doc_id (int): Id of document been added\n",
    "    doc_index (dict): Inverted Index of a specific document.\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"\n",
    "    for word in doc_index.keys():\n",
    "        locations = doc_index[word]\n",
    "        indices = inverted.setdefault(word, {})\n",
    "        indices[doc_id] = locations\n",
    "    return inverted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388570\n"
     ]
    }
   ],
   "source": [
    "inverted_doc_indexes_english = {}\n",
    "files_with_index_english = []\n",
    "files_with_tokens_english = {}\n",
    "for i in tokenized_eng.index:\n",
    "    #Clean and Tokenize text of each document\n",
    "    words = tokenized_eng[i]\n",
    "    #Store tokens\n",
    "    files_with_tokens_english[i] = words\n",
    "\n",
    "    doc_index = inverted_index(words)\n",
    "    inverted_index_add(inverted_doc_indexes_english, i, doc_index)\n",
    "    files_with_index_english.append(i)\n",
    "########################################\n",
    "DF_english = {}\n",
    "for word in inverted_doc_indexes_english.keys():\n",
    "    DF_english[word] = len ([doc for doc in inverted_doc_indexes_english[word]])\n",
    "\n",
    "total_vocab_size_english = len(DF_english)\n",
    "print(total_vocab_size_english)\n",
    "########################################\n",
    "idf_english = {} # Our data structure to store Tf-Idf weights\n",
    "\n",
    "N = len(files_with_tokens_english)\n",
    "\n",
    "for doc_id in tokenized_eng.index:\n",
    "    tokens= tokenized_eng[doc_id]\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):        \n",
    "        # Calculate Idf\n",
    "        if token in DF_english:\n",
    "            df = DF_english[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        # Calculate Tf-idf        \n",
    "        idf_english[token] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139895\n"
     ]
    }
   ],
   "source": [
    "inverted_doc_indexes_german = {}\n",
    "files_with_index_german = []\n",
    "files_with_tokens_german = {}\n",
    "for i in tokenized_ger.index:\n",
    "    #Clean and Tokenize text of each document\n",
    "    words = tokenized_ger[i]\n",
    "    #Store tokens\n",
    "    files_with_tokens_german[i] = words\n",
    "\n",
    "    doc_index = inverted_index(words)\n",
    "    inverted_index_add(inverted_doc_indexes_german, i, doc_index)\n",
    "    files_with_index_german.append(i)\n",
    "\n",
    "################################\n",
    "DF_german = {}\n",
    "for word in inverted_doc_indexes_german.keys():\n",
    "    DF_german[word] = len ([doc for doc in inverted_doc_indexes_german[word]])\n",
    "\n",
    "total_vocab_size_german = len(DF_german)\n",
    "print(total_vocab_size_german)\n",
    "######################\n",
    "idf_german = {} # Our data structure to store Tf-Idf weights\n",
    "\n",
    "N = len(files_with_tokens_german)\n",
    "\n",
    "for doc_id in tokenized_ger.index:\n",
    "    tokens= tokenized_ger[doc_id]\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):        \n",
    "        # Calculate Idf\n",
    "        if token in DF_german:\n",
    "            df = DF_german[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        # Calculate Tf-idf        \n",
    "        idf_german[token] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61734\n"
     ]
    }
   ],
   "source": [
    "inverted_doc_indexes_french = {}\n",
    "files_with_index_french = []\n",
    "files_with_tokens_french = {}\n",
    "for i in tokenized_fre.index:\n",
    "    #Clean and Tokenize text of each document\n",
    "    words = tokenized_fre[i]\n",
    "    #Store tokens\n",
    "    files_with_tokens_french[i] = words\n",
    "\n",
    "    doc_index = inverted_index(words)\n",
    "    inverted_index_add(inverted_doc_indexes_french, i, doc_index)\n",
    "    files_with_index_french.append(i)\n",
    "\n",
    "################################\n",
    "DF_french = {}\n",
    "for word in inverted_doc_indexes_french.keys():\n",
    "    DF_french[word] = len ([doc for doc in inverted_doc_indexes_french[word]])\n",
    "\n",
    "total_vocab_size_french = len(DF_french)\n",
    "print(total_vocab_size_french)\n",
    "######################\n",
    "idf_french = {} # Our data structure to store Tf-Idf weights\n",
    "\n",
    "N = len(files_with_tokens_french)\n",
    "\n",
    "for doc_id in tokenized_fre.index:\n",
    "    tokens= tokenized_fre[doc_id]\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):        \n",
    "        # Calculate Idf\n",
    "        if token in DF_french:\n",
    "            df = DF_french[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        # Calculate Tf-idf        \n",
    "        idf_french[token] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Uni Mannheim\\IR\\Project\\fasttext.py:143: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.dot(vec_a, vec_b) / \\\n"
     ]
    }
   ],
   "source": [
    "#BWE-AGG\n",
    "for queryy in queries:\n",
    "\n",
    "    q_lang = translator.detect(queryy).lang\n",
    "    vec_list_query = [all_dict[q_lang][item]\n",
    "                      for item in queryy.lower().split() if item in all_dict[q_lang]]\n",
    "    vecsum_query = np.sum(vec_list_query, axis=0)\n",
    "\n",
    "    cosine_sim_ger = []\n",
    "    cosine_sim_fre = []\n",
    "    cosine_sim_eng = []\n",
    "    ind_ger = []\n",
    "    ind_fre = []\n",
    "    ind_eng = []\n",
    "\n",
    "    for ix, token_docs in enumerate(tokenized_ger):\n",
    "        ind_ger.append(tokenized_ger.index[ix])\n",
    "        vec_list_doc = [ger_dictionary[item]\n",
    "                        for item in token_docs if item in ger_dictionary]\n",
    "        vecsum_doc = np.sum(vec_list_doc, axis=0)\n",
    "        cosine_sim_ger.append(\n",
    "            FastVector.cosine_similarity(vecsum_query, vecsum_doc))\n",
    "\n",
    "    for ix, token_docs in enumerate(tokenized_eng):\n",
    "        ind_eng.append(tokenized_eng.index[ix])\n",
    "        vec_list_doc = [eng_dictionary[item]\n",
    "                        for item in token_docs if item in eng_dictionary]\n",
    "        vecsum_doc = np.sum(vec_list_doc, axis=0)\n",
    "        cosine_sim_eng.append(\n",
    "            FastVector.cosine_similarity(vecsum_query, vecsum_doc))\n",
    "\n",
    "    for ix, token_docs in enumerate(tokenized_fre):\n",
    "        ind_fre.append(tokenized_fre.index[ix])\n",
    "        vec_list_doc = [fre_dictionary[item]\n",
    "                        for item in token_docs if item in fre_dictionary]\n",
    "        vecsum_doc = np.sum(vec_list_doc, axis=0)\n",
    "        cosine_sim_fre.append(\n",
    "            FastVector.cosine_similarity(vecsum_query, vecsum_doc))\n",
    "\n",
    "    indexx = []\n",
    "\n",
    "    cosine_sim1_ger = [x if isinstance(\n",
    "        x, float) else 0 for x in cosine_sim_ger]\n",
    "    cosine_sim1_fre = [x if isinstance(\n",
    "        x, float) else 0 for x in cosine_sim_fre]\n",
    "    cosine_sim1_eng = [x if isinstance(\n",
    "        x, float) else 0 for x in cosine_sim_eng]\n",
    "    \n",
    "    indexx.append([(_, x) for _, x in sorted(\n",
    "        zip(cosine_sim1_ger, ind_ger))][::-1][:20])\n",
    "    indexx.append([(_, x) for _, x in sorted(\n",
    "        zip(cosine_sim1_fre, ind_fre))][::-1][:20])\n",
    "    indexx.append([(_, x) for _, x in sorted(\n",
    "        zip(cosine_sim1_eng, ind_eng))][::-1][:20])\n",
    "\n",
    "    scores = [oi for oi, ii in sorted(sum(indexx, []))][::-1]\n",
    "    tmp_df = dff.loc[[ii for oi, ii in sorted(sum(indexx, []))][::-1]]\n",
    "    tmp_df['cosine_scores'] = scores\n",
    "    tmp_df.to_csv(\"results/{}.csv\".format(queryy))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BWE-AGG-IDF\n",
    "for queryy in queries:\n",
    "\n",
    "    q_lang = translator.detect(queryy).lang\n",
    "    vec_list_query = [all_dict[q_lang][item]\n",
    "                      for item in queryy.lower().split() if item in all_dict[q_lang]]\n",
    "    vecsum_query = np.sum(vec_list_query, axis=0)\n",
    "\n",
    "    cosine_sim_ger = []\n",
    "    cosine_sim_fre = []\n",
    "    cosine_sim_eng = []\n",
    "    ind_ger = []\n",
    "    ind_fre = []\n",
    "    ind_eng = []\n",
    "\n",
    "    for ix, token_docs in enumerate(tokenized_ger):\n",
    "        ind_ger.append(tokenized_ger.index[ix])\n",
    "        vec_list_doc = [idf_german[item]*ger_dictionary[item]\n",
    "                        for item in token_docs if item in ger_dictionary]\n",
    "        vecsum_doc = np.sum(vec_list_doc, axis=0)\n",
    "        cosine_sim_ger.append(\n",
    "            FastVector.cosine_similarity(vecsum_query, vecsum_doc))\n",
    "\n",
    "    for ix, token_docs in enumerate(tokenized_eng):\n",
    "        ind_eng.append(tokenized_eng.index[ix])\n",
    "        vec_list_doc = [idf_english[item]*eng_dictionary[item]\n",
    "                        for item in token_docs if item in eng_dictionary]\n",
    "        vecsum_doc = np.sum(vec_list_doc, axis=0)\n",
    "        cosine_sim_eng.append(\n",
    "            FastVector.cosine_similarity(vecsum_query, vecsum_doc))\n",
    "\n",
    "    for ix, token_docs in enumerate(tokenized_fre):\n",
    "        ind_fre.append(tokenized_fre.index[ix])\n",
    "        vec_list_doc = [idf_french[item]*fre_dictionary[item]\n",
    "                        for item in token_docs if item in fre_dictionary]\n",
    "        vecsum_doc = np.sum(vec_list_doc, axis=0)\n",
    "        cosine_sim_fre.append(\n",
    "            FastVector.cosine_similarity(vecsum_query, vecsum_doc))\n",
    "\n",
    "    indexx = []\n",
    "\n",
    "    cosine_sim1_ger = [x if isinstance(\n",
    "        x, float) else 0 for x in cosine_sim_ger]\n",
    "    cosine_sim1_fre = [x if isinstance(\n",
    "        x, float) else 0 for x in cosine_sim_fre]\n",
    "    cosine_sim1_eng = [x if isinstance(\n",
    "        x, float) else 0 for x in cosine_sim_eng]\n",
    "\n",
    "    indexx.append([(_, x) for _, x in sorted(\n",
    "        zip(cosine_sim1_ger, ind_ger))][::-1][:20])\n",
    "    indexx.append([(_, x) for _, x in sorted(\n",
    "        zip(cosine_sim1_fre, ind_fre))][::-1][:20])\n",
    "    indexx.append([(_, x) for _, x in sorted(\n",
    "        zip(cosine_sim1_eng, ind_eng))][::-1][:20])\n",
    "    scores = [oi for oi, ii in sorted(sum(indexx, []))][::-1]\n",
    "    tmp_dff = dff.loc[[ii for oi, ii in sorted(sum(indexx, []))][::-1]]\n",
    "    tmp_dff['cosine_scores'] = scores\n",
    "    tmp_dff.to_csv(\"results/{}_idf.csv\".format(queryy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for queryy in queries:\n",
    "    new = pd.read_csv(\"results/{}.csv\".format(queryy))\n",
    "    new_idf = pd.read_csv(\"results/{}_idf.csv\".format(queryy))\n",
    "    final = pd.concat([new, new_idf])\n",
    "    final.drop_duplicates(subset=[\"DBRECORDID\"], inplace=True)\n",
    "    final.to_csv(\"results/{}_pooled.csv\".format(queryy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWE - nDCG : (herzkatheter) 0.7506067814040828\n",
      "BWE-idf - nDCG : (herzkatheter) 0.753409911253676\n",
      "pooled - nDCG : (herzkatheter) 0.8206427022477627\n",
      "BWE - nDCG : (echokardiographie) 0.8244076441144708\n",
      "BWE-idf - nDCG : (echokardiographie) 0.8361264214973626\n",
      "pooled - nDCG : (echokardiographie) 0.8854850021042235\n",
      "BWE - nDCG : (krankenhausplanung) 0.7860122556120882\n",
      "BWE-idf - nDCG : (krankenhausplanung) 0.7876014140033109\n",
      "pooled - nDCG : (krankenhausplanung) 0.8329766684376249\n",
      "BWE - nDCG : (ambient AND assisted AND living AND nursing) 0.7275962057146819\n",
      "BWE-idf - nDCG : (ambient AND assisted AND living AND nursing) 0.7134937797384164\n",
      "pooled - nDCG : (ambient AND assisted AND living AND nursing) 0.7707389881016127\n",
      "BWE - nDCG : (low AND carb) 0.7164431873396453\n",
      "BWE-idf - nDCG : (low AND carb) 0.7373164302628251\n",
      "pooled - nDCG : (low AND carb) 0.806093843600604\n",
      "BWE - nDCG : (épidémiologie) 0.9061277547972217\n",
      "BWE-idf - nDCG : (épidémiologie) 0.9033786592127004\n",
      "pooled - nDCG : (épidémiologie) 0.9299420114805235\n",
      "BWE - nDCG : (alzheimer AND demenz) 0.7346392274869016\n",
      "BWE-idf - nDCG : (alzheimer AND demenz) 0.7189538647686086\n",
      "pooled - nDCG : (alzheimer AND demenz) 0.8302429034397218\n",
      "BWE - nDCG : (fatigue) 0.7401520063643267\n",
      "BWE-idf - nDCG : (fatigue) 0.7329814321051822\n",
      "pooled - nDCG : (fatigue) 0.8335499229866358\n",
      "#######################\n",
      "bwe nDCG score:  0.7732481328541774\n",
      "bwe-idf nDCG score:  0.7729077391052603\n",
      "pooled nDCG score:  0.8387090052998385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "bwe_scores = []\n",
    "bwe_idf_scores = []\n",
    "bwe_pooled_scores = []\n",
    "\n",
    "for queryy in queries:\n",
    "    eval = pd.read_csv(\"ranked_documents/{}_withpoints.csv\".format(queryy))\n",
    "    new = pd.read_csv(\"results/{}.csv\".format(queryy))\n",
    "    new_idf = pd.read_csv(\"results/{}_idf.csv\".format(queryy))\n",
    "\n",
    "    bwe = pd.merge(new,eval[[\"DBRECORDID\",\"Relevance_Point\"]],on=\"DBRECORDID\",how='inner')\n",
    "    bwe_idf = pd.merge(new_idf, eval[[\"DBRECORDID\", \"Relevance_Point\"]], on=\"DBRECORDID\", how='inner')\n",
    "\n",
    "    ranked_relevance = np.asarray([bwe[\"Relevance_Point\"]])\n",
    "    ideal_relevance = np.asarray([(bwe_idf[\"Relevance_Point\"])[::-1]])\n",
    "\n",
    "    ranked_relevance_idf = np.asarray([bwe_idf[\"Relevance_Point\"]])\n",
    "    ideal_relevance_idf = np.asarray([(bwe_idf[\"Relevance_Point\"])[::-1]])\n",
    "\n",
    "    ranked_relevance_pooled = np.asarray([eval[\"Relevance_Point\"]])\n",
    "    ideal_relevance_pooled = np.asarray([eval[\"Relevance_Point\"][::-1]])\n",
    "\n",
    "    bwe_scores.append(ndcg_score(ideal_relevance, ranked_relevance))\n",
    "    bwe_idf_scores.append(ndcg_score(ideal_relevance_idf, ranked_relevance_idf))\n",
    "    bwe_pooled_scores.append(ndcg_score(ideal_relevance_pooled, ranked_relevance_pooled))\n",
    "\n",
    "\n",
    "    print(\"BWE - nDCG : ({})\".format(queryy),ndcg_score(ideal_relevance, ranked_relevance))\n",
    "    print(\"BWE-idf - nDCG : ({})\".format(queryy),ndcg_score(ideal_relevance_idf, ranked_relevance_idf))\n",
    "    print(\"pooled - nDCG : ({})\".format(queryy),ndcg_score(ideal_relevance_pooled, ranked_relevance_pooled))\n",
    "\n",
    "print(\"#######################\")\n",
    "print(\"bwe nDCG score: \",np.mean(bwe_scores))\n",
    "print(\"bwe-idf nDCG score: \", np.mean(bwe_idf_scores))\n",
    "print(\"pooled nDCG score: \", np.mean(bwe_pooled_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bwe nDCG score:  0.7732481328541774\n",
    "\n",
    "bwe-idf nDCG score:  0.7729077391052603\n",
    "\n",
    "pooled nDCG score:  0.8387090052998385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35c754d8008c5f8560e4adf341ebf96f62d30db323e3ac43f60a1cb4dab6d757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
